{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # Specifically designed for deep neural network with the ReLU activation\n",
    "            # that helps to reduce the vanishing gradient problem,\n",
    "            # allows the network to learn deeper representations.\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            # Batch norm layers normalize the distribution of layer outputs during training,\n",
    "            # which reduces the dependency of the deep network on weight initialization strategies.\n",
    "            # The weights and biases of all batch normalization layers are usually initialized as 1s and 0s, respectively.\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            # Specifically designed for deep neural network with the ReLU activation\n",
    "            # that helps to reduce the vanishing gradient problem,\n",
    "            # allows the network to learn deeper representations.\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dropout(model, type, rate=0.2):\n",
    "    dropout_before = \"\"\n",
    "    if type == \"resnet50\":\n",
    "        dropout_before = \"conv3\"\n",
    "        ## Where insert the dropout layer in the bottleneck block\n",
    "        # identity = x\n",
    "        # out = self.conv1(x)\n",
    "        # out = self.bn1(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.conv2(out)\n",
    "        # out = self.bn2(out)\n",
    "        # out = self.relu(out)\n",
    "        ### Dropout2d\n",
    "        # out = self.conv3(out)\n",
    "        # out = self.bn3(out)\n",
    "        # if self.downsample is not None:\n",
    "        #     identity = self.downsample(x)\n",
    "        # out += identity\n",
    "        # out = self.relu(out)\n",
    "    if type == \"resnet18\":\n",
    "        dropout_before = \"conv2\"\n",
    "        ## Where insert the dropout layer in the basic block\n",
    "        # identity = x\n",
    "        # out = self.conv1(x)\n",
    "        # out = self.bn1(out)\n",
    "        # out = self.relu(out)\n",
    "        ### Dropout2d\n",
    "        # out = self.conv2(out)\n",
    "        # out = self.bn2(out)\n",
    "        # if self.downsample is not None:\n",
    "        #     identity = self.downsample(x)\n",
    "        # out += identity\n",
    "        # out = self.relu(out)\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            append_dropout(module, type, rate=rate)\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # inplace=false to avoid the error: one of the variables needed for gradient computation has been modified by an inplace operation\n",
    "            # When we set implace=true we overwrite input tensor (can give error when we use this tensor but use less memory)\n",
    "            # When we set implace=false we work on a copy of tensor (not give error but use more memory)\n",
    "            # Dropout2d before relu: This order encourages the network to learn robust features while maintaining non-linearity.\n",
    "            # Relu before Dropout2d: The idea is to apply dropout after the non-linearity to prevent overfitting on specific features.\n",
    "            # Another research paper suggests that dropout before or after the ReLU layer does not make a significant difference and proof that.\n",
    "            if dropout_before == name:\n",
    "                new = nn.Sequential(nn.Dropout2d(p=rate, inplace=False), module)\n",
    "                setattr(model, name, new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dropout_after_each_relu(model, rate=0.2):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            append_dropout_after_each_relu(module, rate=rate)\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            new = nn.Sequential(module, nn.Dropout2d(p=rate, inplace=False))\n",
    "            setattr(model, name, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dropout_before_each_relu(model, rate=0.2):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            append_dropout_before_each_relu(module, rate=rate)\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            new = nn.Sequential(nn.Dropout2d(p=rate, inplace=False), module)\n",
    "            setattr(model, name, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_architecture(\n",
    "    type=\"resnet50\",\n",
    "    num_classes=200,\n",
    "    pretrained=False,\n",
    "    dropout_rate_bb=0.2,\n",
    "    dropout_rate_fc=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Classification architecture is like a Resnet.\n",
    "\n",
    "    Args:\n",
    "        type (str, optional): Architecture type. Defaults to \"resnet50\".\n",
    "        num_classes (int, optional): Number of output classes. Defaults to 200.\n",
    "        pretrained (bool, optional): Use pretrained model. Defaults to False.\n",
    "        dropout_rate_bb (float, optional): Dropout rate before the fully connected layer. Defaults to 0.2.\n",
    "        dropout_rate_fc (float, optional): Dropout rate before the fully connected layer. Defaults to 0.5.\n",
    "    Returns:\n",
    "        The model with the specified architecture.\n",
    "    \"\"\"\n",
    "    if type != \"resnet18\" and type != \"resnet50\":\n",
    "        type = \"resnet50\"\n",
    "    if pretrained is False:\n",
    "        print(f\"Architecture: not-pretrained {type}\")\n",
    "        model = torch.hub.load(\"pytorch/vision:v0.17.0\", type)\n",
    "        model.apply(init_weights)\n",
    "    else:\n",
    "        print(f\"Architecture: pretrained {type}\")\n",
    "        model = torch.hub.load(\"pytorch/vision:v0.17.0\", type, weights=\"IMAGENET1K_V1\")\n",
    "    \n",
    "    # remove maxpool\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    if dropout_rate_bb > 0:\n",
    "        # You can choose one of the following three options\n",
    "        append_dropout(model, type, rate=dropout_rate_bb)\n",
    "        # append_dropout_after_each_relu(model, rate=dropout_rate_bb)\n",
    "        # append_dropout_before_each_relu(model, rate=dropout_rate_bb)\n",
    "        \n",
    "    # define a new head for the detector with required number of classes\n",
    "    if dropout_rate_fc > 0:\n",
    "        avgpool = model.avgpool\n",
    "        model.avgpool = nn.Sequential(nn.Dropout(dropout_rate_fc), avgpool)\n",
    "        # model.avgpool = nn.Sequential(avgpool, nn.Dropout(dropout_rate_fc))\n",
    "    in_features = model.fc.in_features\n",
    "    fc = nn.Linear(in_features, num_classes)\n",
    "    if num_classes == 1:\n",
    "        fc = nn.Sequential(fc, nn.Sigmoid())\n",
    "    model.fc = fc\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: not-pretrained resnet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vince/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): Identity()\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Sequential(\n",
      "        (0): Dropout2d(p=0.2, inplace=False)\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(get_nn_architecture(\n",
    "    type=\"resnet18\",\n",
    "    num_classes=200,\n",
    "    pretrained=False,\n",
    "    dropout_rate_bb=0.2,\n",
    "    dropout_rate_fc=0.5,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
